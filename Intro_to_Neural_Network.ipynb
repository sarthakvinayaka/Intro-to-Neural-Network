{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro to Neural Network.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe_0zdRHVt9O",
        "colab_type": "text"
      },
      "source": [
        "##**Building Blocks**\n",
        "\n",
        "First, we have to talk about neurons, the basic unit of a neural network. A neuron takes inputs, does some math with them, and produces one output. Here’s what a 2-input neuron looks like:\n",
        "\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1200/1*JRRC_UDsW1kDgPK3MW1GjQ.png)\n",
        "\n",
        "\n",
        "3 things are happening here. First, each input is multiplied by a weight:\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1400/1*Iq76QGqSTJfYRztFhwK0yw.png)\n",
        "\n",
        "\n",
        "Next, all the weighted inputs are added together with a bias b:\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1400/1*CE-YfWhFQ2yQSGq9Zaxd9Q.png)\n",
        "\n",
        "\n",
        "\n",
        "Finally, the sum is passed through an activation function:\n",
        "\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1400/1*9BFMXPkoAqN_EW7XTPvuGg.png)\n",
        "\n",
        "The activation function is used to turn an unbounded input into an output that has a nice, predictable form. A commonly used activation function is the sigmoid function:\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1400/1*Ul8Yu_r8GKSFillzbPFrPQ.png)\n",
        "\n",
        "The sigmoid function only outputs numbers in the range (0,1). You can think of it as compressing (−∞,+∞) to (0,1) — big negative numbers become ~0, and big positive numbers become ~1.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEJdn070Y8dV",
        "colab_type": "text"
      },
      "source": [
        "##**An Example: Feedforward**\n",
        "\n",
        "Let’s use the network pictured above and assume all neurons have the same weights w=[0,1], the same bias b=0, and the same sigmoid activation function. Let h1​, h2​, o1​ denote the outputs of the neurons they represent.\n",
        "\n",
        "\n",
        "What happens if we pass in the input x=[2, 3]?\n",
        "\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1400/1*6oNvFWIYvAxH0whBFCxsKw.png)\n",
        "\n",
        "\n",
        "The output of the neural network for input x=[2,3] is 0.7216.\n",
        "\n",
        "\n",
        "A neural network can have any number of layers with any number of neurons in those layers. The basic idea stays the same: feed the input(s) forward through the neurons in the network to get the output(s) at the end.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgFkPztwZ2pn",
        "colab_type": "text"
      },
      "source": [
        "##**Loss**\n",
        "\n",
        "Before we train our network, we first need a way to quantify how “good” it’s doing so that it can try to do “better”. That’s what the loss is.\n",
        "\n",
        "\n",
        "We’ll use the mean squared error (MSE) loss:\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1400/1*AGjwUIJ62a9He2K919OJug.png)\n",
        "\n",
        "Let’s break this down:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   n is the number of samples.\n",
        "*   y represents the variable being predicted\n",
        "*   y_true​ is the true value of the variable (the “correct answer”).\n",
        "*   y_pred​ is the predicted value of the variable\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "(y_true​−y_pred​)² is known as the squared error.Our loss function is simply taking the average over all squared errors (hence the name mean squared error. The better our predictions are, the lower our loss will be!\n",
        "\n",
        "Better predictions = Lower loss.\n",
        "\n",
        "Training a network = trying to minimize its loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xFkqiB5bpH7",
        "colab_type": "text"
      },
      "source": [
        "##**Training a Neural Network**\n",
        "\n",
        "We now have a clear goal: minimize the loss of the neural network. We know we can change the network’s weights and biases to influence its predictions.\n",
        "\n",
        "Another way to think about loss is as a function of weights and biases. Let’s label each weight and bias in our network:\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1100/1*JuCFYUaqd7WTX8PKHkfuQw.png)\n",
        "\n",
        "\n",
        "Then, we can write loss as a multivariable function:\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1400/1*OHMn7EMtIG77EAmccwgowg.png)\n",
        "\n",
        "\n",
        "We’ll use an optimization algorithm called stochastic gradient descent (SGD) that tells us how to change our weights and biases to minimize loss. \n",
        "\n",
        "![alt text](https://miro.medium.com/max/1400/1*kX2Av8AoG8VX42kXhhFHZw.png)\n",
        "\n",
        "\n",
        "η is a constant called the learning rate that controls how fast we train. All we’re doing is subtracting η ∂w1/​∂L​ from w1​:\n",
        "\n",
        "\n",
        "If we do this for every weight and bias in the network, the loss will slowly decrease and our network will improve.\n",
        "\n",
        "\n",
        "Our training process will look like this:\n",
        "\n",
        "\n",
        "\n",
        "1.  Choose one sample from our dataset.\n",
        "2.  Calculate all the partial derivatives of loss with respect to weights or biases (e.g. ∂L/∂w1​, ∂L​/∂w2​, etc).\n",
        "\n",
        "3. Use the update equation to update each weight and bias.\n",
        "\n",
        "4. Go back to step 1.\n",
        "\n",
        "\n",
        "Now lets implement it using tensorflow where keras has inbuilt library which will perform all the calculation for us.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Oi93NoydJKW",
        "colab_type": "text"
      },
      "source": [
        "##**Implementation using Tensorflow**\n",
        "\n",
        "We have used heart disease dataset for binary classification. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Puas7PsVajS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "97ff6d3a-9f2f-426c-a096-82637f869c03"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from pandas import set_option"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a23uSzlwdg2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('/content/heart.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLfkHTdsdk0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7d5c81ea-a7ce-4612-ed55-f3f5e9756065"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
              "0   63    1   3       145   233    1  ...      0      2.3      0   0     1       1\n",
              "1   37    1   2       130   250    0  ...      0      3.5      0   0     2       1\n",
              "2   41    0   1       130   204    0  ...      0      1.4      2   0     2       1\n",
              "3   56    1   1       120   236    0  ...      0      0.8      2   0     2       1\n",
              "4   57    0   0       120   354    0  ...      1      0.6      2   0     2       1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpMrImqsdnBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=data.drop('target',axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIwkt1tPdpSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = data[\"target\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M_DzcHvdq9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test,Y_train,Y_test = train_test_split(x,y,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48n_w3sgdu7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(1,activation='sigmoid',input_dim=13))\n",
        "model.compile(loss='binary_crossentropy',optimizer='SGD',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6trWeplZeTUQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b788a427-b964-4f35-c221-c542c8c9dd01"
      },
      "source": [
        "history=model.fit(X_train, Y_train, epochs=50, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 227 samples, validate on 76 samples\n",
            "Epoch 1/50\n",
            "227/227 [==============================] - 0s 241us/step - loss: 113.9708 - accuracy: 0.5242 - val_loss: 90.7369 - val_accuracy: 0.5395\n",
            "Epoch 2/50\n",
            "227/227 [==============================] - 0s 56us/step - loss: 114.9075 - accuracy: 0.5110 - val_loss: 250.4163 - val_accuracy: 0.5395\n",
            "Epoch 3/50\n",
            "227/227 [==============================] - 0s 50us/step - loss: 114.7991 - accuracy: 0.5419 - val_loss: 93.4201 - val_accuracy: 0.5395\n",
            "Epoch 4/50\n",
            "227/227 [==============================] - 0s 52us/step - loss: 142.9154 - accuracy: 0.4670 - val_loss: 16.6715 - val_accuracy: 0.6842\n",
            "Epoch 5/50\n",
            "227/227 [==============================] - 0s 57us/step - loss: 100.4753 - accuracy: 0.5286 - val_loss: 238.1895 - val_accuracy: 0.5395\n",
            "Epoch 6/50\n",
            "227/227 [==============================] - 0s 56us/step - loss: 125.1347 - accuracy: 0.5022 - val_loss: 217.2588 - val_accuracy: 0.5395\n",
            "Epoch 7/50\n",
            "227/227 [==============================] - 0s 55us/step - loss: 108.7867 - accuracy: 0.5286 - val_loss: 150.6380 - val_accuracy: 0.4605\n",
            "Epoch 8/50\n",
            "227/227 [==============================] - 0s 58us/step - loss: 117.2531 - accuracy: 0.5286 - val_loss: 319.0828 - val_accuracy: 0.5395\n",
            "Epoch 9/50\n",
            "227/227 [==============================] - 0s 57us/step - loss: 120.7906 - accuracy: 0.5595 - val_loss: 16.6773 - val_accuracy: 0.6974\n",
            "Epoch 10/50\n",
            "227/227 [==============================] - 0s 57us/step - loss: 82.5073 - accuracy: 0.5419 - val_loss: 198.3976 - val_accuracy: 0.5395\n",
            "Epoch 11/50\n",
            "227/227 [==============================] - 0s 59us/step - loss: 69.1129 - accuracy: 0.5947 - val_loss: 26.5384 - val_accuracy: 0.6711\n",
            "Epoch 12/50\n",
            "227/227 [==============================] - 0s 57us/step - loss: 137.6162 - accuracy: 0.4714 - val_loss: 212.2371 - val_accuracy: 0.5395\n",
            "Epoch 13/50\n",
            "227/227 [==============================] - 0s 51us/step - loss: 111.3080 - accuracy: 0.5198 - val_loss: 129.6335 - val_accuracy: 0.4605\n",
            "Epoch 14/50\n",
            "227/227 [==============================] - 0s 48us/step - loss: 87.7366 - accuracy: 0.5507 - val_loss: 295.2326 - val_accuracy: 0.4605\n",
            "Epoch 15/50\n",
            "227/227 [==============================] - 0s 53us/step - loss: 126.0803 - accuracy: 0.5154 - val_loss: 17.4575 - val_accuracy: 0.6842\n",
            "Epoch 16/50\n",
            "227/227 [==============================] - 0s 59us/step - loss: 66.9233 - accuracy: 0.5947 - val_loss: 33.2689 - val_accuracy: 0.6316\n",
            "Epoch 17/50\n",
            "227/227 [==============================] - 0s 56us/step - loss: 66.0643 - accuracy: 0.5815 - val_loss: 50.0112 - val_accuracy: 0.5789\n",
            "Epoch 18/50\n",
            "227/227 [==============================] - 0s 55us/step - loss: 97.5581 - accuracy: 0.5374 - val_loss: 116.0817 - val_accuracy: 0.5395\n",
            "Epoch 19/50\n",
            "227/227 [==============================] - 0s 52us/step - loss: 114.0204 - accuracy: 0.5066 - val_loss: 48.7024 - val_accuracy: 0.6447\n",
            "Epoch 20/50\n",
            "227/227 [==============================] - 0s 54us/step - loss: 79.3792 - accuracy: 0.5507 - val_loss: 76.2687 - val_accuracy: 0.5526\n",
            "Epoch 21/50\n",
            "227/227 [==============================] - 0s 58us/step - loss: 105.1845 - accuracy: 0.5066 - val_loss: 110.9712 - val_accuracy: 0.5395\n",
            "Epoch 22/50\n",
            "227/227 [==============================] - 0s 53us/step - loss: 102.9457 - accuracy: 0.5198 - val_loss: 56.3667 - val_accuracy: 0.5921\n",
            "Epoch 23/50\n",
            "227/227 [==============================] - 0s 51us/step - loss: 109.7980 - accuracy: 0.4978 - val_loss: 245.7286 - val_accuracy: 0.5395\n",
            "Epoch 24/50\n",
            "227/227 [==============================] - 0s 52us/step - loss: 80.2150 - accuracy: 0.5991 - val_loss: 26.5675 - val_accuracy: 0.6711\n",
            "Epoch 25/50\n",
            "227/227 [==============================] - 0s 53us/step - loss: 95.0252 - accuracy: 0.5639 - val_loss: 23.0139 - val_accuracy: 0.7105\n",
            "Epoch 26/50\n",
            "227/227 [==============================] - 0s 58us/step - loss: 49.2498 - accuracy: 0.6079 - val_loss: 206.8398 - val_accuracy: 0.5395\n",
            "Epoch 27/50\n",
            "227/227 [==============================] - 0s 62us/step - loss: 85.3099 - accuracy: 0.5551 - val_loss: 199.9982 - val_accuracy: 0.5395\n",
            "Epoch 28/50\n",
            "227/227 [==============================] - 0s 53us/step - loss: 94.6088 - accuracy: 0.5771 - val_loss: 43.7880 - val_accuracy: 0.6184\n",
            "Epoch 29/50\n",
            "227/227 [==============================] - 0s 52us/step - loss: 97.8273 - accuracy: 0.5463 - val_loss: 71.2440 - val_accuracy: 0.5526\n",
            "Epoch 30/50\n",
            "227/227 [==============================] - 0s 56us/step - loss: 66.6077 - accuracy: 0.5991 - val_loss: 134.5983 - val_accuracy: 0.4605\n",
            "Epoch 31/50\n",
            "227/227 [==============================] - 0s 56us/step - loss: 96.2610 - accuracy: 0.5551 - val_loss: 63.5316 - val_accuracy: 0.5658\n",
            "Epoch 32/50\n",
            "227/227 [==============================] - 0s 54us/step - loss: 77.5221 - accuracy: 0.5727 - val_loss: 270.2393 - val_accuracy: 0.5395\n",
            "Epoch 33/50\n",
            "227/227 [==============================] - 0s 52us/step - loss: 83.7301 - accuracy: 0.6564 - val_loss: 181.7454 - val_accuracy: 0.5395\n",
            "Epoch 34/50\n",
            "227/227 [==============================] - 0s 54us/step - loss: 114.6182 - accuracy: 0.4978 - val_loss: 30.8575 - val_accuracy: 0.6711\n",
            "Epoch 35/50\n",
            "227/227 [==============================] - 0s 53us/step - loss: 98.7639 - accuracy: 0.5022 - val_loss: 372.9405 - val_accuracy: 0.4605\n",
            "Epoch 36/50\n",
            "227/227 [==============================] - 0s 58us/step - loss: 126.5620 - accuracy: 0.5198 - val_loss: 127.5882 - val_accuracy: 0.5395\n",
            "Epoch 37/50\n",
            "227/227 [==============================] - 0s 56us/step - loss: 82.8906 - accuracy: 0.5815 - val_loss: 40.5866 - val_accuracy: 0.6711\n",
            "Epoch 38/50\n",
            "227/227 [==============================] - 0s 53us/step - loss: 84.8583 - accuracy: 0.5507 - val_loss: 112.5132 - val_accuracy: 0.5526\n",
            "Epoch 39/50\n",
            "227/227 [==============================] - 0s 73us/step - loss: 84.3647 - accuracy: 0.5463 - val_loss: 118.5433 - val_accuracy: 0.5526\n",
            "Epoch 40/50\n",
            "227/227 [==============================] - 0s 54us/step - loss: 97.3374 - accuracy: 0.5419 - val_loss: 18.3948 - val_accuracy: 0.7632\n",
            "Epoch 41/50\n",
            "227/227 [==============================] - 0s 65us/step - loss: 61.0536 - accuracy: 0.6035 - val_loss: 205.9260 - val_accuracy: 0.4605\n",
            "Epoch 42/50\n",
            "227/227 [==============================] - 0s 54us/step - loss: 100.0289 - accuracy: 0.5639 - val_loss: 53.2502 - val_accuracy: 0.6184\n",
            "Epoch 43/50\n",
            "227/227 [==============================] - 0s 52us/step - loss: 93.5236 - accuracy: 0.5507 - val_loss: 141.1884 - val_accuracy: 0.4605\n",
            "Epoch 44/50\n",
            "227/227 [==============================] - 0s 52us/step - loss: 97.3702 - accuracy: 0.5551 - val_loss: 240.1858 - val_accuracy: 0.4605\n",
            "Epoch 45/50\n",
            "227/227 [==============================] - 0s 56us/step - loss: 82.0885 - accuracy: 0.5727 - val_loss: 85.2021 - val_accuracy: 0.5526\n",
            "Epoch 46/50\n",
            "227/227 [==============================] - 0s 55us/step - loss: 80.2087 - accuracy: 0.5903 - val_loss: 31.6609 - val_accuracy: 0.6447\n",
            "Epoch 47/50\n",
            "227/227 [==============================] - 0s 56us/step - loss: 78.5541 - accuracy: 0.5683 - val_loss: 156.1046 - val_accuracy: 0.5395\n",
            "Epoch 48/50\n",
            "227/227 [==============================] - 0s 62us/step - loss: 108.5934 - accuracy: 0.5374 - val_loss: 66.0954 - val_accuracy: 0.5789\n",
            "Epoch 49/50\n",
            "227/227 [==============================] - 0s 68us/step - loss: 59.9256 - accuracy: 0.6256 - val_loss: 29.5872 - val_accuracy: 0.6711\n",
            "Epoch 50/50\n",
            "227/227 [==============================] - 0s 58us/step - loss: 56.6655 - accuracy: 0.6035 - val_loss: 21.4250 - val_accuracy: 0.7368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNtir3rxfPsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f983540b-1c36-44d7-f8b5-ba68f7812124"
      },
      "source": [
        "model.predict_classes(X_test,verbose=0)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8OggfMPggzx",
        "colab_type": "text"
      },
      "source": [
        "##**Conclusion**\n",
        "\n",
        "We have succesfully buid our Neural Network model using tensorflow.\n"
      ]
    }
  ]
}